{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo Notebook for Tone Grabber"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Generator Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the effects and effect parameter mappings for the dataset generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.data_generator import DataGenerator\n",
    "from pedalboard import Reverb, Delay, Chorus, Distortion, Gain\n",
    "# Dictionary of effects to parameter mappings\n",
    "effects_to_parameters = {\n",
    "    \"Reverb\": {\n",
    "        \"room_size\": (0, 1),\n",
    "        \"damping\": (0, 1), \n",
    "        \"wet_level\": (0, 1),\n",
    "        \"dry_level\": (0, 1),\n",
    "        \"width\": (0, 1),\n",
    "        \"freeze_mode\": (0, 1)\n",
    "    },\n",
    "    \"Delay\": {\n",
    "        \"delay_seconds\": (0, 2),\n",
    "        \"feedback\": (0, 1),\n",
    "        \"mix\": (0, 1)\n",
    "    },\n",
    "    \"Chorus\": {\n",
    "        \"rate_hz\": (0, 100),\n",
    "        \"depth\": (0, 1),\n",
    "        \"centre_delay_ms\": (1, 30),\n",
    "        \"feedback\": (0, 1),\n",
    "        \"mix\": (0, 1)\n",
    "    },\n",
    "    \"Distortion\": {\n",
    "        \"drive_db\": (0, 100)\n",
    "    },\n",
    "    \"Gain\": {\n",
    "        \"gain_db\": (-12, 12)\n",
    "    }\n",
    "}\n",
    "# List of effects\n",
    "effects = [Reverb, Delay, Distortion, Gain, Chorus]\n",
    "\n",
    "# create instance of data generator corresponding to effects\n",
    "generator = DataGenerator(effects_to_parameters, effects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a demo dataset with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# num samples is the number of samples created per audio effect so total number of samples created will be:\n",
    "# num_samples * number of dry_tones\n",
    "num_samples = 2\n",
    "audio_directory = os.path.join(os.getcwd(),\"demo_data\")\n",
    "dry_tones = os.listdir(audio_directory)\n",
    "# max_chain_length is the maximum number of effects applied to a sample\n",
    "max_chain_length = 1\n",
    "demo_dataset = generator.create_data(num_samples,audio_directory,dry_tones,max_chain_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each entry of the dataset has this output signature:\n",
    "\n",
    "```\n",
    "\"dry_tone\": \n",
    "{\n",
    "    \"spectrogram\":log mel spectrogram of the dry tone,\n",
    "    \"loudness\":loudness of the dry tone,\n",
    "    \"f0\":fundamental frequency of the dry tone,\n",
    "    \"path\":path to the original dry tone\n",
    "}\n",
    "```\n",
    "\n",
    "```\n",
    "\"wet_tone\": {\n",
    "    \"spectrogram\":log mel spectrogram of the wet tone,\n",
    "    \"loudness\":loudness of the wet tone\n",
    "    \"f0\":fundamental frequency of the wet tone,\n",
    "    \"path\":path to the original wet tone\n",
    "}\n",
    "```\n",
    "```\n",
    "\"effect_names\":names of the applied effect(s)\n",
    "```\n",
    "```\n",
    "\"effects\":one-hot encoding representation of the effects\n",
    "```\n",
    "```\n",
    "\"parameters\": one-hot like representation of the effect parameters\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(demo_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also get the metadata for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = generator.get_metadata()\n",
    "display(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extractor Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature extractor is built into the data generator class so it runs automatically when you run ```generator.create_data()``` \n",
    "\n",
    "But here is some demo code in case you run into problems using it anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pedalboard.io import ReadableAudioFile\n",
    "from dataset.feature_extractor_torch import FeatureExtractorTorch\n",
    "import numpy as np\n",
    "# define instance of feature extractor\n",
    "feature_extractor = FeatureExtractorTorch()\n",
    "sample_rate = 16000\n",
    "# read in audio path\n",
    "dry_tone_path = \"demo_data/guitar_acoustic_017-102-050.wav\"\n",
    "with ReadableAudioFile(dry_tone_path) as f:\n",
    "    # re sample the audio file to match the sample rate, pretrained model is sampled at 16000\n",
    "    re_sampled = f.resampled_to(sample_rate)\n",
    "    dry_tone = np.squeeze(re_sampled.read(int(sample_rate * f.duration)),axis=0)\n",
    "    re_sampled.close()\n",
    "    f.close()\n",
    "# read in features\n",
    "features = feature_extractor.get_features(dry_tone)\n",
    "# features extracted are log mel spectrogram, loudness, and fundamental frequency (f0)\n",
    "display(features['spectrogram'].shape)\n",
    "display(features['loudness'].shape)\n",
    "display(features['f0'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Prediction Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.parameter_prediction import ParameterPrediction\n",
    "param_mask = metadata['parameter_mask_idx']\n",
    "num_parameters = metadata['total_parameters']\n",
    "num_effects = len(metadata['effect_to_idx'].keys())\n",
    "model = ParameterPrediction(num_effects,num_parameters,param_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entry = demo_dataset[0]\n",
    "wet_tone_feat = entry[\"wet_tone\"]\n",
    "dry_tone_feat = entry[\"dry_tone\"]\n",
    "\n",
    "out,  effect, params = model(wet_tone_feat['spectrogram'],dry_tone_feat['spectrogram'],wet_tone_feat['loudness'],wet_tone_feat['f0'],dry_tone_feat['loudness'],dry_tone_feat['f0'])\n",
    "display(out.shape)\n",
    "display(effect)\n",
    "display(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolate output into pedalboad effect matched with parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first get the pedalboard effect object from the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "predicted_effect_pb = metadata['effects'][int(torch.argmax(effect))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then match pedicted parameters to their names so  we can use them as input to the effect object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_params = [float(param) for param in list(params[0].detach()) if param != 0]\n",
    "param_names = metadata['effects_to_parameters'][metadata['index_to_effect'][int(torch.argmax(effect))]].keys()\n",
    "matched_params = {param_name:value for param_name,value in zip(param_names,predicted_params)}\n",
    "matched_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can use this on the dry tone create the pedicted wet tone, since this model has random weights it will likely not be accurate rright now but hopefully it will be when trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pedalboard import Pedalboard\n",
    "pred = predicted_effect_pb(**matched_params)\n",
    "pedalboard = Pedalboard([pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pedalboard.io import ReadableAudioFile\n",
    "import numpy as np\n",
    "with ReadableAudioFile(entry['dry_tone']['path']) as f:\n",
    "    re_sampled = f.resampled_to(sample_rate)\n",
    "    dry_tone = np.squeeze(re_sampled.read(int(sample_rate * f.duration)),axis=0)\n",
    "    re_sampled.close()\n",
    "    f.close()\n",
    "wet_tone = pedalboard(dry_tone, sample_rate * f.duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicted wet tone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "ipd.Audio(data=wet_tone,rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the parameter prediction model on a sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.parameter_prediction import Trainer\n",
    "from auraloss.freq import MultiResolutionSTFTLoss\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(demo_dataset, batch_size=2, shuffle=True)\n",
    "loss_fn = MultiResolutionSTFTLoss()\n",
    "optimizer = Adam(model.parameters(),.0001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.25)\n",
    "trainer = Trainer(model, metadata)\n",
    "trainer.train(model, train_loader, train_loader, loss_fn, optimizer, scheduler, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    print(batch['dry_tone'])\n",
    "    print(batch['wet_tone'])\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tone-grabber-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
