{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonat/neural-effect-chain/tone-grabber-venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from model.EffectDecoder import EffectDecoder\n",
    "from transformers import ASTModel\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EffectClassifier(torch.nn.Module):\n",
    "    def __init__(self, n_classes,embed_dim=768):\n",
    "        super(EffectClassifier, self).__init__()\n",
    "        self.pretrained = ASTModel.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
    "        self.embed = torch.nn.Linear(embed_dim,embed_dim)\n",
    "        self.cls = torch.nn.Linear(embed_dim, n_classes)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.softmax = torch.nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pretrained(**x).pooler_output\n",
    "        x = self.relu(self.embed(x))\n",
    "        x = self.cls(x)\n",
    "        return self.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pickle.load(open(\"data/guitar_sample_dataset_multiclass.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dry_tone_path': 'data/instrument_dataset/Train_submission/Train_submission/1-E1-Major 00.wav',\n",
       " 'wet_tone_path': 'data/wet_tones/1-E1-Major 00_wet_0.wav',\n",
       " 'wet_tone_features': {'input_values': tensor([[[-1.2310, -1.2776, -1.1612,  ..., -1.2776, -1.2776, -1.2776],\n",
       "          [-1.1927, -1.2776, -1.0318,  ..., -1.2776, -1.2776, -1.2776],\n",
       "          [-1.0118, -1.2629, -0.8861,  ..., -1.2776, -1.2776, -1.2776],\n",
       "          ...,\n",
       "          [ 0.4670,  0.4670,  0.4670,  ...,  0.4670,  0.4670,  0.4670],\n",
       "          [ 0.4670,  0.4670,  0.4670,  ...,  0.4670,  0.4670,  0.4670],\n",
       "          [ 0.4670,  0.4670,  0.4670,  ...,  0.4670,  0.4670,  0.4670]]])},\n",
       " 'effect_names': ['Gain'],\n",
       " 'effects': tensor([[0., 0., 0., 1., 0.]]),\n",
       " 'parameters': tensor([[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "            0.0000,   0.0000, -17.1298,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "            0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "            0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "            0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "            0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "            0.0000,   0.0000,   0.0000]]),\n",
       " 'index': 0}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(dataset, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, loss_fn, dl):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    labels = []\n",
    "    preds = []\n",
    "    for batch in tqdm.tqdm(dl):\n",
    "        features = batch['wet_tone_features'].to(device)\n",
    "        label = batch['effects'].to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(features)\n",
    "        loss = loss_fn(output, label)\n",
    "        total_loss += loss.item()\n",
    "        preds.append(torch.argmax(output, dim=-1).cpu().numpy())\n",
    "        labels.append(torch.argmax(label).cpu().numpy())\n",
    "    print(f\"Accuracy:{accuracy_score(labels, preds)} | Total Loss:{total_loss}\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, train_loader,test_loader, epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in tqdm.tqdm(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            features = batch['wet_tone_features'].to(device)\n",
    "            labels = batch['effects'].to(device)\n",
    "            output = model(features)\n",
    "            loss = loss_fn(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss}\")\n",
    "        eval(model, loss_fn, test_loader)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EffectClassifier(5).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=.0000005)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [01:34<00:00,  4.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 627.8390402793884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:06<00:00, 15.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.56 | Total Loss:152.8575165271759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [01:34<00:00,  4.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 582.9692931175232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:06<00:00, 15.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.67 | Total Loss:140.95802009105682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [01:32<00:00,  4.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 537.9732059836388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:06<00:00, 15.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.7 | Total Loss:132.32380890846252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [01:41<00:00,  3.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 509.97031432390213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:06<00:00, 15.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.73 | Total Loss:127.62350732088089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [01:29<00:00,  4.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 492.1641817688942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:05<00:00, 16.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.74 | Total Loss:124.5223405957222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, loss_fn, train_data, test_data, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1730742580.129816  279245 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5520 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import crepe\n",
    "from ddsp.spectral_ops import compute_loudness, stft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1805/2185664882.py:4: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  sr, audio = wavfile.read('data/dry_tones/Electric1.wav')\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1730761482.262423   16273 service.cc:148] XLA service 0x7fc3d800b690 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1730761482.263291   16273 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 4070 Laptop GPU, Compute Capability 8.9\n",
      "2024-11-04 17:04:42.298456: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1730761482.457039   16273 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "2024-11-04 17:04:50.179877: E external/local_xla/xla/service/slow_operation_alarm.cc:65] Trying algorithm eng11{k2=4,k3=0} for conv (f32[32,128,128,1]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,1024,191,1]{3,2,1,0}, f32[128,1024,64,1]{3,2,1,0}, f32[128]{0}), window={size=64x1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kRelu\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]} is taking a while...\n",
      "2024-11-04 17:04:50.234272: E external/local_xla/xla/service/slow_operation_alarm.cc:133] The operation took 6.110339848s\n",
      "Trying algorithm eng11{k2=4,k3=0} for conv (f32[32,128,128,1]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,1024,191,1]{3,2,1,0}, f32[128,1024,64,1]{3,2,1,0}, f32[128]{0}), window={size=64x1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kRelu\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]} is taking a while...\n",
      "2024-11-04 17:04:50.234592: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:306] Allocator (GPU_0_bfc) ran out of memory trying to allocate 80.67GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-11-04 17:04:44.773608: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:306] Allocator (GPU_0_bfc) ran out of memory trying to allocate 17.31GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 7/19\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1730761485.271276   16273 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/19\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-04 17:04:46.602068: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:306] Allocator (GPU_0_bfc) ran out of memory trying to allocate 74.62GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-11-04 17:04:47.572371: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:306] Allocator (GPU_0_bfc) ran out of memory trying to allocate 16.93GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 151ms/step\n"
     ]
    }
   ],
   "source": [
    "import crepe\n",
    "from scipy.io import wavfile\n",
    "\n",
    "sr, audio = wavfile.read('data/dry_tones/Electric1.wav')\n",
    "time, frequency, confidence, activation = crepe.predict(audio, sr, viterbi=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tf2onnx\n",
    "tf2onnx.convert.from_keras(model, output_path='model.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(596,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5334961 , 0.6296221 , 0.8698378 , 0.93597   , 0.9421039 ,\n",
       "       0.94829744, 0.9376753 , 0.93722534, 0.9454341 , 0.9401556 ,\n",
       "       0.94271576, 0.94477797, 0.9450136 , 0.94556755, 0.9531479 ,\n",
       "       0.94853216, 0.9408216 , 0.9497386 , 0.9543725 , 0.9439076 ,\n",
       "       0.9441276 , 0.95267344, 0.9476809 , 0.9511987 , 0.9556084 ,\n",
       "       0.9535724 , 0.95312613, 0.9589498 , 0.955145  , 0.9472393 ,\n",
       "       0.9572332 , 0.9571534 , 0.9460222 , 0.9498669 , 0.95371455,\n",
       "       0.9439485 , 0.9469752 , 0.9478384 , 0.9500707 , 0.9501963 ,\n",
       "       0.9503716 , 0.948262  , 0.94317514, 0.95212287, 0.94810647,\n",
       "       0.93393356, 0.9448331 , 0.9501995 , 0.9366317 , 0.9421654 ,\n",
       "       0.9387107 , 0.9382332 , 0.9428285 , 0.9382944 , 0.9340488 ,\n",
       "       0.9387747 , 0.9382097 , 0.9353085 , 0.9322601 , 0.9364581 ,\n",
       "       0.9396669 , 0.92812693, 0.9326183 , 0.9353108 , 0.9333069 ,\n",
       "       0.93721473, 0.93084145, 0.9314979 , 0.94108725, 0.93568885,\n",
       "       0.93526834, 0.9308967 , 0.93698275, 0.9380307 , 0.925094  ,\n",
       "       0.9338252 , 0.9368218 , 0.93122107, 0.93643135, 0.9275088 ,\n",
       "       0.9309308 , 0.93972456, 0.93064356, 0.9304879 , 0.92955726,\n",
       "       0.93405837, 0.9341159 , 0.92150515, 0.93319154, 0.93519664,\n",
       "       0.9298476 , 0.93510705, 0.9248419 , 0.92847157, 0.93826944,\n",
       "       0.9281489 , 0.92753345, 0.9302286 , 0.9337136 , 0.93297875,\n",
       "       0.9217225 , 0.93440646, 0.9323486 , 0.9271687 , 0.93076307,\n",
       "       0.9204186 , 0.92516416, 0.93507165, 0.92567474, 0.9242597 ,\n",
       "       0.9271775 , 0.92973566, 0.92516214, 0.9129826 , 0.9260961 ,\n",
       "       0.91571563, 0.91770095, 0.9224521 , 0.90928096, 0.9127197 ,\n",
       "       0.9155965 , 0.90876126, 0.90366894, 0.90077376, 0.90825385,\n",
       "       0.8941973 , 0.88763386, 0.9017971 , 0.8666527 , 0.87552655,\n",
       "       0.8797474 , 0.8518228 , 0.8588333 , 0.8463297 , 0.8428456 ,\n",
       "       0.8350383 , 0.83085537, 0.8517907 , 0.81498754, 0.8482402 ,\n",
       "       0.87227666, 0.83944184, 0.8717589 , 0.8906979 , 0.8860768 ,\n",
       "       0.897206  , 0.9024807 , 0.9098051 , 0.90459794, 0.9110142 ,\n",
       "       0.92149603, 0.90831155, 0.9091649 , 0.91576   , 0.9054996 ,\n",
       "       0.91446114, 0.9214241 , 0.9134176 , 0.9192036 , 0.92184645,\n",
       "       0.9271313 , 0.9156609 , 0.91861945, 0.9285389 , 0.92217904,\n",
       "       0.9222424 , 0.9249902 , 0.91833186, 0.9287292 , 0.9306511 ,\n",
       "       0.92295563, 0.9264104 , 0.92894065, 0.9352479 , 0.926234  ,\n",
       "       0.9261049 , 0.9368447 , 0.9323176 , 0.93427163, 0.93046904,\n",
       "       0.92295307, 0.93048555, 0.9280447 , 0.92084634, 0.9232607 ,\n",
       "       0.9265921 , 0.9357872 , 0.92836165, 0.92381805, 0.9323319 ,\n",
       "       0.92927796, 0.93229896, 0.9281627 , 0.9201527 , 0.9289251 ,\n",
       "       0.93072855, 0.92593735, 0.92524135, 0.9271256 , 0.9381359 ,\n",
       "       0.9308344 , 0.9232298 , 0.92856205, 0.9253251 , 0.93119645,\n",
       "       0.9296081 , 0.91963047, 0.928148  , 0.9280189 , 0.92654455,\n",
       "       0.9197771 , 0.9199654 , 0.93176454, 0.92117494, 0.917436  ,\n",
       "       0.920363  , 0.91611093, 0.920066  , 0.9172553 , 0.9101066 ,\n",
       "       0.9167063 , 0.914231  , 0.9127928 , 0.90748805, 0.910338  ,\n",
       "       0.91639704, 0.8967749 , 0.90703744, 0.90499026, 0.8995633 ,\n",
       "       0.9018013 , 0.8975885 , 0.8925815 , 0.89736104, 0.8932513 ,\n",
       "       0.8914813 , 0.87603045, 0.8935224 , 0.89651257, 0.8731567 ,\n",
       "       0.8892599 , 0.89287484, 0.8906521 , 0.8880559 , 0.8904676 ,\n",
       "       0.89133686, 0.89420265, 0.9029925 , 0.9025689 , 0.88902843,\n",
       "       0.9040189 , 0.9097847 , 0.90169835, 0.9107287 , 0.9089517 ,\n",
       "       0.913199  , 0.91353935, 0.91742694, 0.91736144, 0.9177866 ,\n",
       "       0.92860377, 0.9279675 , 0.9214831 , 0.92378336, 0.93203306,\n",
       "       0.92704475, 0.93434   , 0.92884827, 0.93219477, 0.937391  ,\n",
       "       0.9344758 , 0.9292765 , 0.9287575 , 0.9378707 , 0.93471426,\n",
       "       0.93087953, 0.9326765 , 0.93628937, 0.9339556 , 0.9383097 ,\n",
       "       0.93374926, 0.9363801 , 0.94006276, 0.93904245, 0.9327763 ,\n",
       "       0.93413347, 0.9392101 , 0.93690145, 0.9335308 , 0.9341584 ,\n",
       "       0.93601775, 0.9355163 , 0.9390787 , 0.9361861 , 0.9361573 ,\n",
       "       0.94032   , 0.94013023, 0.9337134 , 0.9337979 , 0.93542635,\n",
       "       0.9336875 , 0.9322811 , 0.92923516, 0.9325845 , 0.93405753,\n",
       "       0.93282056, 0.9276837 , 0.9257762 , 0.9290118 , 0.9303069 ,\n",
       "       0.92128927, 0.9205463 , 0.92094743, 0.9193727 , 0.9197318 ,\n",
       "       0.9108759 , 0.9181432 , 0.91935766, 0.9158911 , 0.9128417 ,\n",
       "       0.9043732 , 0.91357696, 0.9108258 , 0.9029971 , 0.9020809 ,\n",
       "       0.89741814, 0.904967  , 0.9004289 , 0.8938848 , 0.8997981 ,\n",
       "       0.89195704, 0.889551  , 0.89026475, 0.8778981 , 0.88637435,\n",
       "       0.871952  , 0.86951286, 0.87403184, 0.8700898 , 0.88420784,\n",
       "       0.87470233, 0.8837799 , 0.8865321 , 0.8784322 , 0.88631517,\n",
       "       0.8867588 , 0.877509  , 0.8901011 , 0.873268  , 0.8787746 ,\n",
       "       0.8844231 , 0.87845296, 0.8905645 , 0.88289225, 0.8835431 ,\n",
       "       0.88516015, 0.8768063 , 0.8808851 , 0.8746936 , 0.8697947 ,\n",
       "       0.88255066, 0.8650128 , 0.8695649 , 0.87696284, 0.8683256 ,\n",
       "       0.877629  , 0.8714143 , 0.8719392 , 0.88016355, 0.86746466,\n",
       "       0.8763952 , 0.8688166 , 0.8637017 , 0.8793633 , 0.8580552 ,\n",
       "       0.8689781 , 0.8720893 , 0.8622591 , 0.86864966, 0.8610915 ,\n",
       "       0.86149776, 0.86511445, 0.84818804, 0.8574499 , 0.84434175,\n",
       "       0.8419599 , 0.85488814, 0.83239573, 0.83977836, 0.8513289 ,\n",
       "       0.8391917 , 0.84314406, 0.83739775, 0.84551877, 0.8433358 ,\n",
       "       0.8251904 , 0.851075  , 0.8261395 , 0.8378301 , 0.8456445 ,\n",
       "       0.8234288 , 0.8350384 , 0.84800327, 0.834111  , 0.8389499 ,\n",
       "       0.83687747, 0.84285223, 0.8455282 , 0.8210989 , 0.85060775,\n",
       "       0.8292028 , 0.8515398 , 0.85086155, 0.8300185 , 0.8542447 ,\n",
       "       0.85826653, 0.8490812 , 0.85355216, 0.85220957, 0.8603491 ,\n",
       "       0.8639445 , 0.84313583, 0.8660432 , 0.8452352 , 0.86460143,\n",
       "       0.8601017 , 0.83929193, 0.86053175, 0.8611722 , 0.85581064,\n",
       "       0.85436356, 0.8511327 , 0.86201745, 0.86293346, 0.85092217,\n",
       "       0.86767435, 0.85006905, 0.86975974, 0.8694262 , 0.8534542 ,\n",
       "       0.86854696, 0.87136644, 0.86877537, 0.86581767, 0.8611833 ,\n",
       "       0.87379014, 0.8712172 , 0.8615805 , 0.87670875, 0.8539601 ,\n",
       "       0.8733807 , 0.8748318 , 0.8530462 , 0.86165965, 0.85980946,\n",
       "       0.8639523 , 0.856549  , 0.8501745 , 0.8580325 , 0.84988713,\n",
       "       0.853897  , 0.86252415, 0.83198005, 0.8557848 , 0.86528516,\n",
       "       0.8454944 , 0.84659916, 0.84641486, 0.86074686, 0.85557383,\n",
       "       0.84762645, 0.8528443 , 0.8392115 , 0.853774  , 0.8608626 ,\n",
       "       0.8330624 , 0.8474741 , 0.86182976, 0.84603506, 0.83733594,\n",
       "       0.8335701 , 0.84815985, 0.84692794, 0.8372387 , 0.8462516 ,\n",
       "       0.82379514, 0.84720105, 0.857903  , 0.83630687, 0.83264893,\n",
       "       0.8547998 , 0.8563364 , 0.8381177 , 0.8350199 , 0.8512302 ,\n",
       "       0.8514382 , 0.8454496 , 0.85737216, 0.8292267 , 0.85136193,\n",
       "       0.8663605 , 0.8507995 , 0.8444527 , 0.8598967 , 0.87362957,\n",
       "       0.8525285 , 0.8551441 , 0.8648646 , 0.86604035, 0.87021977,\n",
       "       0.88286835, 0.8616918 , 0.8725183 , 0.8880784 , 0.88488287,\n",
       "       0.87534684, 0.8835922 , 0.8978272 , 0.88434076, 0.8870279 ,\n",
       "       0.89180344, 0.8852659 , 0.89223063, 0.89743483, 0.8858957 ,\n",
       "       0.88695794, 0.90004295, 0.89900863, 0.8891204 , 0.89857006,\n",
       "       0.90666246, 0.9017963 , 0.9048923 , 0.910437  , 0.9066486 ,\n",
       "       0.9088199 , 0.91409355, 0.911227  , 0.9056848 , 0.9182749 ,\n",
       "       0.91677225, 0.91164577, 0.9148745 , 0.91826785, 0.9160275 ,\n",
       "       0.91693735, 0.9173075 , 0.9143246 , 0.91178364, 0.91703403,\n",
       "       0.9142678 , 0.9003879 , 0.914184  , 0.9147569 , 0.90644294,\n",
       "       0.9060789 , 0.9056669 , 0.9069213 , 0.9037919 , 0.89954364,\n",
       "       0.8976749 , 0.8948749 , 0.9071629 , 0.9037965 , 0.8922689 ,\n",
       "       0.9075363 , 0.90855724, 0.8990718 , 0.8986297 , 0.8917622 ,\n",
       "       0.9017315 , 0.8938905 , 0.89031553, 0.88068736, 0.87648284,\n",
       "       0.88760364, 0.8785007 , 0.87353915, 0.8735731 , 0.8751313 ,\n",
       "       0.86340606, 0.8604956 , 0.85190856, 0.8632936 , 0.85215807,\n",
       "       0.84734803, 0.8473469 , 0.84717405, 0.8586758 , 0.85504746,\n",
       "       0.8553751 , 0.85669506, 0.8472079 , 0.7966924 , 0.6425203 ,\n",
       "       0.5004796 ], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.  , 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1 ,\n",
       "       0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2 , 0.21,\n",
       "       0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3 , 0.31, 0.32,\n",
       "       0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4 , 0.41, 0.42, 0.43,\n",
       "       0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5 , 0.51, 0.52, 0.53, 0.54,\n",
       "       0.55, 0.56, 0.57, 0.58, 0.59, 0.6 , 0.61, 0.62, 0.63, 0.64, 0.65,\n",
       "       0.66, 0.67, 0.68, 0.69, 0.7 , 0.71, 0.72, 0.73, 0.74, 0.75, 0.76,\n",
       "       0.77, 0.78, 0.79, 0.8 , 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87,\n",
       "       0.88, 0.89, 0.9 , 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98,\n",
       "       0.99, 1.  , 1.01, 1.02, 1.03, 1.04, 1.05, 1.06, 1.07, 1.08, 1.09,\n",
       "       1.1 , 1.11, 1.12, 1.13, 1.14, 1.15, 1.16, 1.17, 1.18, 1.19, 1.2 ,\n",
       "       1.21, 1.22, 1.23, 1.24, 1.25, 1.26, 1.27, 1.28, 1.29, 1.3 , 1.31,\n",
       "       1.32, 1.33, 1.34, 1.35, 1.36, 1.37, 1.38, 1.39, 1.4 , 1.41, 1.42,\n",
       "       1.43, 1.44, 1.45, 1.46, 1.47, 1.48, 1.49, 1.5 , 1.51, 1.52, 1.53,\n",
       "       1.54, 1.55, 1.56, 1.57, 1.58, 1.59, 1.6 , 1.61, 1.62, 1.63, 1.64,\n",
       "       1.65, 1.66, 1.67, 1.68, 1.69, 1.7 , 1.71, 1.72, 1.73, 1.74, 1.75,\n",
       "       1.76, 1.77, 1.78, 1.79, 1.8 , 1.81, 1.82, 1.83, 1.84, 1.85, 1.86,\n",
       "       1.87, 1.88, 1.89, 1.9 , 1.91, 1.92, 1.93, 1.94, 1.95, 1.96, 1.97,\n",
       "       1.98, 1.99, 2.  , 2.01, 2.02, 2.03, 2.04, 2.05, 2.06, 2.07, 2.08,\n",
       "       2.09, 2.1 , 2.11, 2.12, 2.13, 2.14, 2.15, 2.16, 2.17, 2.18, 2.19,\n",
       "       2.2 , 2.21, 2.22, 2.23, 2.24, 2.25, 2.26, 2.27, 2.28, 2.29, 2.3 ,\n",
       "       2.31, 2.32, 2.33, 2.34, 2.35, 2.36, 2.37, 2.38, 2.39, 2.4 , 2.41,\n",
       "       2.42, 2.43, 2.44, 2.45, 2.46, 2.47, 2.48, 2.49, 2.5 , 2.51, 2.52,\n",
       "       2.53, 2.54, 2.55, 2.56, 2.57, 2.58, 2.59, 2.6 , 2.61, 2.62, 2.63,\n",
       "       2.64, 2.65, 2.66, 2.67, 2.68, 2.69, 2.7 , 2.71, 2.72, 2.73, 2.74,\n",
       "       2.75, 2.76, 2.77, 2.78, 2.79, 2.8 , 2.81, 2.82, 2.83, 2.84, 2.85,\n",
       "       2.86, 2.87, 2.88, 2.89, 2.9 , 2.91, 2.92, 2.93, 2.94, 2.95, 2.96,\n",
       "       2.97, 2.98, 2.99, 3.  , 3.01, 3.02, 3.03, 3.04, 3.05, 3.06, 3.07,\n",
       "       3.08, 3.09, 3.1 , 3.11, 3.12, 3.13, 3.14, 3.15, 3.16, 3.17, 3.18,\n",
       "       3.19, 3.2 , 3.21, 3.22, 3.23, 3.24, 3.25, 3.26, 3.27, 3.28, 3.29,\n",
       "       3.3 , 3.31, 3.32, 3.33, 3.34, 3.35, 3.36, 3.37, 3.38, 3.39, 3.4 ,\n",
       "       3.41, 3.42, 3.43, 3.44, 3.45, 3.46, 3.47, 3.48, 3.49, 3.5 , 3.51,\n",
       "       3.52, 3.53, 3.54, 3.55, 3.56, 3.57, 3.58, 3.59, 3.6 , 3.61, 3.62,\n",
       "       3.63, 3.64, 3.65, 3.66, 3.67, 3.68, 3.69, 3.7 , 3.71, 3.72, 3.73,\n",
       "       3.74, 3.75, 3.76, 3.77, 3.78, 3.79, 3.8 , 3.81, 3.82, 3.83, 3.84,\n",
       "       3.85, 3.86, 3.87, 3.88, 3.89, 3.9 , 3.91, 3.92, 3.93, 3.94, 3.95,\n",
       "       3.96, 3.97, 3.98, 3.99, 4.  , 4.01, 4.02, 4.03, 4.04, 4.05, 4.06,\n",
       "       4.07, 4.08, 4.09, 4.1 , 4.11, 4.12, 4.13, 4.14, 4.15, 4.16, 4.17,\n",
       "       4.18, 4.19, 4.2 , 4.21, 4.22, 4.23, 4.24, 4.25, 4.26, 4.27, 4.28,\n",
       "       4.29, 4.3 , 4.31, 4.32, 4.33, 4.34, 4.35, 4.36, 4.37, 4.38, 4.39,\n",
       "       4.4 , 4.41, 4.42, 4.43, 4.44, 4.45, 4.46, 4.47, 4.48, 4.49, 4.5 ,\n",
       "       4.51, 4.52, 4.53, 4.54, 4.55, 4.56, 4.57, 4.58, 4.59, 4.6 , 4.61,\n",
       "       4.62, 4.63, 4.64, 4.65, 4.66, 4.67, 4.68, 4.69, 4.7 , 4.71, 4.72,\n",
       "       4.73, 4.74, 4.75, 4.76, 4.77, 4.78, 4.79, 4.8 , 4.81, 4.82, 4.83,\n",
       "       4.84, 4.85, 4.86, 4.87, 4.88, 4.89, 4.9 , 4.91, 4.92, 4.93, 4.94,\n",
       "       4.95, 4.96, 4.97, 4.98, 4.99, 5.  , 5.01, 5.02, 5.03, 5.04, 5.05,\n",
       "       5.06, 5.07, 5.08, 5.09, 5.1 , 5.11, 5.12, 5.13, 5.14, 5.15, 5.16,\n",
       "       5.17, 5.18, 5.19, 5.2 , 5.21, 5.22, 5.23, 5.24, 5.25, 5.26, 5.27,\n",
       "       5.28, 5.29, 5.3 , 5.31, 5.32, 5.33, 5.34, 5.35, 5.36, 5.37, 5.38,\n",
       "       5.39, 5.4 , 5.41, 5.42, 5.43, 5.44, 5.45, 5.46, 5.47, 5.48, 5.49,\n",
       "       5.5 , 5.51, 5.52, 5.53, 5.54, 5.55, 5.56, 5.57, 5.58, 5.59, 5.6 ,\n",
       "       5.61, 5.62, 5.63, 5.64, 5.65, 5.66, 5.67, 5.68, 5.69, 5.7 , 5.71,\n",
       "       5.72, 5.73, 5.74, 5.75, 5.76, 5.77, 5.78, 5.79, 5.8 , 5.81, 5.82,\n",
       "       5.83, 5.84, 5.85, 5.86, 5.87, 5.88, 5.89, 5.9 , 5.91, 5.92, 5.93,\n",
       "       5.94, 5.95])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tone-grabber-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
